# Built with Opus 4.6: Claude Code Hackathon — Full Context

## Event Basics
- **Dates:** Feb 10–16, 2026 (1 week)
- **Format:** Virtual | 500 selected from 13,000+ applicants (~4% acceptance)
- **Team size:** Max 2 (solo or pair)
- **Credits:** $500 API credits per participant
- **Open source required:** Yes, everything must be open source
- **No prior work:** All projects must start from scratch
- **Submission:** GitHub repo + project description + **video (most important)**

---

## Problem Statements

### PS1: Build a Tool That Should Exist
> Create the AI-native app or workflow you wish someone had already built. Eliminate busywork. Make hard things effortless.

**Example projects:**
- Contract Lifecycle Autopilot — Extract obligations from contracts, track deadlines, auto-reminders
- Product Changelog Publisher — Release notes → customer-facing announcements across channels
- Bug Report Enricher — Auto-add system logs, user history, reproduction steps to support tickets

**Theme:** Productivity, automation, eliminating busywork. AI-native = couldn't exist without AI.

### PS2: Break the Barriers
> Expert knowledge, essential tools, AI's benefits — take something powerful that's locked behind expertise, cost, language, or infrastructure and put it in everyone's hands.

**Example projects:**
- Crop Doctor — Image analysis + weather + soil → diagnose plant diseases, recommend organic treatment
- Accessibility Auditor — Evaluate websites/docs/physical spaces against accessibility standards + remediation plans
- Open Source Hardware Guide — Navigate component selection, PCB design, manufacturing

**Theme:** Democratization. Make expert-level capability accessible to non-experts.

### PS3: Amplify Human Judgment
> Build AI that makes researchers, professionals, and decision-makers dramatically more capable — without taking them out of the loop. The best AI doesn't replace human expertise. It sharpens it.

**Example projects:**
- Brand Safety Monitor — Review ad placements, flag reputation risks for marketing teams
- Discovery Anomaly Detector — Flag documents that should exist but are missing based on cross-references
- Grading Calibration Partner — Highlight scoring inconsistencies, supporting not overriding professional judgment

**Theme:** Human-in-the-loop. AI as force multiplier for domain experts, not a replacement.

---

## Judging Criteria (Weighted)

### 1. Demo — 30% (HIGHEST WEIGHT)
> Is this a working, impressive demo? Does it hold up live? Is it genuinely cool to watch?

**Implication:** The demo IS the project. A polished, working demo beats a feature-complete but boring one. Must be visually compelling, smooth, and "wow"-inducing in a video.

### 2. Impact — 25%
> What's the real-world potential? Who benefits, and how much does it matter? Could this actually become something people use? Does it fit one of the problem statements?

**Implication:** Must solve a real problem for real people. Judges want to see something that could become a product, not just a tech demo.

### 3. Opus 4.6 Use — 25%
> How creatively did this team use Opus 4.6? Did they go beyond basic integration? Did they surface capabilities that surprised even us?

**Implication:** This is CRITICAL. Must showcase what Opus 4.6 specifically can do that other models can't. Key differentiators:
- 1M token context window (beta) — feed massive documents/codebases
- 128K output tokens — generate extensive, structured output
- Complex reasoning — multi-step, nuanced analysis
- Extended thinking — deep reasoning chains
- Code review & debugging — superior code understanding
- Tool use — agentic workflows

### 4. Depth & Execution — 20%
> Did the team push past their first idea? Is the engineering sound and thoughtfully refined? Does this feel like something that was wrestled with — real craft, not just a quick hack?

**Implication:** Iteration matters. Show that you explored alternatives, refined the approach, and built something with engineering rigor. Quality > quantity of features.

---

## Special Prizes (Opportunity for bonus wins)

### Most Creative Opus 4.6 Exploration — $5K
> The team that found the most interesting edge of this new model — the unexpected capability or the use case nobody thought to try. Taught us something new about what Opus 4.6 can do.

**Strategy:** Find a surprising, non-obvious use of Opus 4.6's capabilities. Something that makes the judges say "we didn't know it could do that."

### The "Keep Thinking" Prize — $5K
> The project that didn't stop at the first idea. Pushed past the obvious, iterated relentlessly, showed depth that turns a good hack into something genuinely surprising.

**Strategy:** Document your iteration journey. Show evolution from v1 → v2 → v3. The final product should feel deeply considered.

---

## Judging Panel
- **Boris Cherny** — Claude Code team
- **Cat Wu** — Claude Code team
- **Thariq Shihpar** — Claude Code team
- **Lydia Hallie** — Claude Code team (known for developer education, web dev)
- **Ado Kukic** — Claude Code team (known for developer relations, DevRel)
- **Jason Bigman** — Claude Code team

---

## Key Strategic Insights

### What wins (derived from criteria weights):
1. **Demo is king (30%)** — Invest heavily in a smooth, impressive demo video
2. **Real impact (25%)** — Solve a genuine problem people care about
3. **Creative Opus 4.6 use (25%)** — Don't just use Claude as a chatbot. Push its unique capabilities
4. **Show your craft (20%)** — Refined engineering, not a weekend hack feeling

### What Opus 4.6 brings that others don't:
- **1M token context** — Process entire codebases, book-length documents, massive datasets in one shot
- **128K output** — Generate complete applications, comprehensive reports, full documentation
- **Superior reasoning** — Complex multi-step analysis, nuanced judgment
- **Extended thinking** — Deep reasoning chains visible to the user
- **Code mastery** — Review, debug, and understand complex code deeply

### Anti-patterns to avoid:
- Basic chatbot wrapper
- Simple RAG application (overdone)
- Anything that could work equally well with GPT-4 or Gemini
- Feature bloat over demo polish
- No real-world use case (pure tech demo)

### Winning formula:
**Real problem + Creative Opus 4.6 leverage + Polished demo + Visible iteration depth**
